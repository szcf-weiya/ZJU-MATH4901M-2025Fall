# Seminars On Frontier Mathematics 前沿数学专题讨论


Here is the introduction of the course:

> This is a course for senior students to guide students to discover the frontier research problems of mathematics, applied mathematics and **statistics**, to understand the background and needs of theory and applied research, to broaden students’ horizons in the field of scientific research, and to encourage students to devote themselves to scientific research and work in the related fields of high-level applied mathematics in the future.

In this course, students are required to read and present research papers. 

The selected topic for my class is:

**Topics on Statistics for Large Language Models (LLMs)**

Here is a curated reading list.

## Lecture 1: Introduction

- Ji, W., Yuan, W., Getzen, E., Cho, K., Jordan, M. I., Mei, S., Weston, J. E., Su, W. J., Xu, J., & Zhang, L. (2025). An Overview of Large Language Models for Statisticians (No. arXiv:2502.17814). arXiv. https://doi.org/10.48550/arXiv.2502.17814

## Lecture 2: Conformal Prediction

- Angelopoulos, A. N., & Bates, S. (2022). A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (No. arXiv:2107.07511). arXiv. https://doi.org/10.48550/arXiv.2107.07511
- Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-Free Predictive Inference for Regression. Journal of the American Statistical Association, 113(523), 1094–1111. https://doi.org/10.1080/01621459.2017.1307116
- Barber, R. F., Candès, E. J., Ramdas, A., & Tibshirani, R. J. (2021). Predictive inference with the jackknife+. The Annals of Statistics, 49(1), 486–507. https://doi.org/10.1214/20-AOS1965
- Cherian, J., Gibbs, I., & Candes, E. (2024, November 6). Large language model validity via enhanced conformal prediction methods. The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=JD3NYpeQ3R

## Lecture 3: Fairness

- Kim, M. P., Ghorbani, A., & Zou, J. (2018). Multiaccuracy: Black-Box Post-Processing for Fairness in Classification (No. arXiv:1805.12317). arXiv. https://doi.org/10.48550/arXiv.1805.12317
- Deng, Z., Zhang, J., Zhang, L., Ye, T., Coley, Y., Su, W. J., & Zou, J. (2022). FIFA: Making Fairness More Generalizable in Classifiers Trained on Imbalanced Data (No. arXiv:2206.02792). arXiv. https://doi.org/10.48550/arXiv.2206.02792
- Zhang, L., Roth, A., & Zhang, L. (2024). Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks. Proceedings of the 41st International Conference on Machine Learning, 59783–59805. https://proceedings.mlr.press/v235/zhang24be.html

## Lecture 4: Watermarking

- Li, X., Ruan, F., Wang, H., Long, Q., & Su, W. J. (2025). A statistical framework of watermarks for large language models: Pivot, detection efficiency and optimal rules. The Annals of Statistics, 53(1), 322–351. https://doi.org/10.1214/24-AOS2468
- Xie, Y., Li, X., Mallick, T., Su, W., & Zhang, R. (2025). Debiasing Watermarks for Large Language Models via Maximal Coupling. Journal of the American Statistical Association, 0(0), 1–11. https://doi.org/10.1080/01621459.2025.2520455
- Li, X., Ruan, F., Wang, H., Long, Q., & Su, W. J. (2025). Robust detection of watermarks for large language models under human edits. Journal of the Royal Statistical Society Series B, qkaf056, https://doi.org/10.1093/jrsssb/qkaf056

## Lecture 5: Beyond Fairness -- Black-box Inference

- Angelopoulos, A. N., Bates, S., Fannjiang, C., Jordan, M. I., & Zrnic, T. (2023). Prediction-powered inference. Science, 382(6671), 669–674. https://doi.org/10.1126/science.adi6000
- Zrnic, T., & Candès, E. J. (2024). Cross-prediction-powered inference. Proceedings of the National Academy of Sciences, 121(15), e2322083121. https://doi.org/10.1073/pnas.2322083121
- Miao, J., Miao, X., Wu, Y., Zhao, J., & Lu, Q. (2025). Assumption-lean and data-adaptive post-prediction inference. Journal of Machine Learning Research, 26(179), 1–31. https://www.jmlr.org/papers/v26/24-0056.html

## Lecture 6: Beyond Watermarking -- Higher Criticism

- Donoho, D., & Jin, J. (2004). Higher criticism for detecting sparse heterogeneous mixtures. The Annals of Statistics, 32(3), 962–994. https://doi.org/10.1214/009053604000000265
- Donoho, D., & Jin, J. (2015). Higher Criticism for Large-Scale Inference, Especially for Rare and Weak Effects. Statistical Science, 30(1), 1–25. https://doi.org/10.1214/14-STS506
- Li, J., & Siegmund, D. (2015). Higher criticism: $p$-values and criticism. The Annals of Statistics, 43(3). https://doi.org/10.1214/15-AOS1312

## Lecture 7: Alignment

- Liu, K., Long, Q., Shi, Z., Su, W. J., & Xiao, J. (2025). Statistical Impossibility and Possibility of Aligning LLMs with Human Preferences: From Condorcet Paradox to Nash Equilibrium (No. arXiv:2503.10990). arXiv. https://doi.org/10.48550/arXiv.2503.10990
- Li, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., & Luo, Z.-Q. (2024). ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models (No. arXiv:2310.10505). arXiv. https://doi.org/10.48550/arXiv.2310.10505
- Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2024). Direct Preference Optimization: Your Language Model is Secretly a Reward Model (No. arXiv:2305.18290). arXiv. https://doi.org/10.48550/arXiv.2305.18290

## Lecture 8: Beyond Higher Criticism -- Multiple Hypothesis Testing

- Barnett, I., Mukherjee, R., & Lin, X. (2017). The Generalized Higher Criticism for Testing SNP-Set Effects in Genetic Association Studies. Journal of the American Statistical Association, 112(517), 64–76. https://doi.org/10.1080/01621459.2016.1192039
- Liu, Y., & Xie, J. (2020). Cauchy Combination Test: A Powerful Test With Analytic p-Value Calculation Under Arbitrary Dependency Structures. Journal of the American Statistical Association, 115(529), 393–402. https://doi.org/10.1080/01621459.2018.1554485
- Vesely, A., Finos, L., & Goeman, J. J. (2023). Permutation-based true discovery guarantee by sum tests. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(3), 664–683. https://doi.org/10.1093/jrsssb/qkad019

## Lecture 9: Beyond Alignment -- Reinforcement Learning

- Sutton, R. S., & Barto, A. (2020). Reinforcement learning: An introduction (Second edition). The MIT Press. http://incompleteideas.net/book/RLbook2020.pdf
- https://github.com/zhoubolei/introRL
- https://github.com/ucla-rlcourse/RLexample

## Lecture 10: Generative AI

- Kotelnikov, A., Baranchuk, D., Rubachev, I., & Babenko, A. (2023). TabDDPM: Modelling Tabular Data with Diffusion Models. Proceedings of the 40th International Conference on Machine Learning, 17564–17579. https://proceedings.mlr.press/v202/kotelnikov23a.html
- Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models (No. arXiv:2006.11239). arXiv. https://doi.org/10.48550/arXiv.2006.11239
- Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations (No. arXiv:2011.13456). arXiv. https://doi.org/10.48550/arXiv.2011.13456
- Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., & Lakshminarayanan, B. (2021). Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning Research, 22(57), 1–64.
- CVPR2022 tutorial（https://cvpr2022-tutorial-diffusion-models.github.io/)
- MIT course (https://diffusion.csail.mit.edu/)

## Lecture 11: Synthetic Data (Help or Hype?)

- Shen, X., Liu, Y., & Shen, R. (2024). Boosting Data Analytics With Synthetic Volume Expansion (No. arXiv:2310.17848). arXiv. https://doi.org/10.48550/arXiv.2310.17848
- Tian, X., & Shen, X. (2025). Enhancing Accuracy in Generative Models via Knowledge Transfer (No. arXiv:2405.16837). arXiv. https://doi.org/10.48550/arXiv.2405.16837
- Jain, A., Montanari, A., & Sasoglu, E. (2024, November 6). Scaling laws for learning with real and surrogate data. The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=NAcHv7vtL2&noteId=J92zK59XDi

## Lecture 12: Privacy

- Su, W. J. (2025). A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation, and Blackwell’s Theorem. Annual Review of Statistics and Its Application, 12(1), 157–175. https://doi.org/10.1146/annurev-statistics-112723-034158
- Yu, D., Naik, S., Backurs, A., Gopi, S., Inan, H. A., Kamath, G., Kulkarni, J., Lee, Y. T., Manoel, A., Wutschitz, L., Yekhanin, S., & Zhang, H. (2022). Differentially Private Fine-tuning of Language Models (No. arXiv:2110.06500). arXiv. https://doi.org/10.48550/arXiv.2110.06500
- Dong, J., Roth, A., & Su, W. J. (2022). Gaussian Differential Privacy. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1), 3–37. https://doi.org/10.1111/rssb.12454
- Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016). Deep Learning with Differential Privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308–318. https://doi.org/10.1145/2976749.2978318
- Bu, Z., Dong, J., Long, Q., & Su, W. (2020). Deep Learning With Gaussian Differential Privacy. Harvard Data Science Review, 2(3). https://doi.org/10.1162/99608f92.cfc5dd25

## Lecture 13: Model Collapse

- Shumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Anderson, R., & Gal, Y. (2024). AI models collapse when trained on recursively generated data. Nature, 631(8022), 755–759. https://doi.org/10.1038/s41586-024-07566-y
- Gerstgrasser, M., Schaeffer, R., Dey, A., Rafailov, R., Sleight, H., Hughes, J., Korbak, T., Agrawal, R., Pai, D., Gromov, A., Roberts, D. A., Yang, D., Donoho, D. L., & Koyejo, S. (2024). Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data (No. arXiv:2404.01413). arXiv. https://doi.org/10.48550/arXiv.2404.01413
- Dohmatob, E., Feng, Y., & Kempe, J. (2024). Model Collapse Demystified: The Case of Regression. Advances in Neural Information Processing Systems, 37, 46979–47013.
